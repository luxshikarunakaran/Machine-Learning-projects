{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1YNmp80CpoDkYJRfa_na4qWNe4tlq_Jyr","authorship_tag":"ABX9TyMA1xMuBZMcKMSUu4o/yg6q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"9Bad8OwG08rU","executionInfo":{"status":"ok","timestamp":1742380772036,"user_tz":-330,"elapsed":8247,"user":{"displayName":"luxshi karunakaran","userId":"14981122548573112690"}}},"outputs":[],"source":["from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import ConcatDataset\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","from torch import nn, optim\n","import torch"]},{"cell_type":"code","source":["base_folder = \"/content/drive/MyDrive/Data\"\n","train_folder = 'train'\n","validation_folder = 'valid'\n","test_folder = 'test'\n","\n","train_path = base_folder + '/' + train_folder\n","validation_path = base_folder + '/' + validation_folder\n","test_path = base_folder + '/' + test_folder\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","augment_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomRotation(degrees=15),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n","    transform\n","])\n","\n","train_data = datasets.ImageFolder(root=train_path, transform=transform)\n","augmented_data = datasets.ImageFolder(root=train_path, transform=augment_transform)\n","combined_train_data = ConcatDataset([train_data, augmented_data])\n","\n","valid_data = datasets.ImageFolder(root=validation_path, transform=transform)\n","test_data = datasets.ImageFolder(root=test_path, transform=transform)"],"metadata":{"id":"80dP-L5e1At6","executionInfo":{"status":"ok","timestamp":1742380773760,"user_tz":-330,"elapsed":109,"user":{"displayName":"luxshi karunakaran","userId":"14981122548573112690"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["train_batch_size = 100\n","valid_batch_size = 9\n","\n","train_dataloader = DataLoader(combined_train_data, batch_size=train_batch_size, shuffle=True)\n","valid_dataloader = DataLoader(valid_data, batch_size=valid_batch_size)\n","test_dataloader = DataLoader(test_data, batch_size=1)"],"metadata":{"id":"xAY1Cy8K1VRR","executionInfo":{"status":"ok","timestamp":1742380776928,"user_tz":-330,"elapsed":35,"user":{"displayName":"luxshi karunakaran","userId":"14981122548573112690"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class CustomResNet(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CustomResNet, self).__init__()\n","        self.resnet = models.resnet50(weights='ResNet50_Weights.IMAGENET1K_V2')\n","        num_features = self.resnet.fc.in_features\n","        self.resnet.fc = nn.Identity()\n","        self.classifier = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Linear(num_features, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        x = self.classifier(x)\n","        return x\n","\n","class AveragingModel(nn.Module):\n","    def __init__(self,models):\n","        super(AveragingModel, self).__init__()\n","        self.models = models\n","\n","    def forward(self,x):\n","        outputs = [torch.sigmoid(model(x)) for model in self.models]\n","        # outputs = [torch.softmax(model(x), dim=1) for model in self.models]\n","        stacked_outputs = torch.stack(outputs)\n","        averaged_output = torch.mean(stacked_outputs, dim=0)\n","        return averaged_output"],"metadata":{"id":"w34_80d81ZgJ","executionInfo":{"status":"ok","timestamp":1742380778041,"user_tz":-330,"elapsed":7,"user":{"displayName":"luxshi karunakaran","userId":"14981122548573112690"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def train(dataloader,model,loss_fn,optimizer):\n","    for X, y in dataloader:\n","        model.train()\n","\n","        y_pred = model(X)\n","        loss = loss_fn(y_pred, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    return loss.item()\n","\n","def test(dataloader,model,loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    loss, accuracy = 0, 0\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            y_pred = model(X)\n","            loss += loss_fn(y_pred, y).item()\n","            accuracy += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n","    loss /= num_batches\n","    accuracy /= size\n","    return loss, accuracy\n","\n","def recompute_batch_statistics(model, dataloader):\n","    model.train()\n","    with torch.no_grad():\n","        for X, _ in dataloader:\n","            model(X)"],"metadata":{"id":"f5lDKHNO1cNQ","executionInfo":{"status":"ok","timestamp":1742380780501,"user_tz":-330,"elapsed":31,"user":{"displayName":"luxshi karunakaran","userId":"14981122548573112690"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["model = CustomResNet(4)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","optimizer1 = optim.AdamW(params=model.parameters(), lr=0.0005)\n","scheduler1 = CosineAnnealingLR(optimizer1, T_max=10, eta_min=0.00001)\n","best_valid_loss = float('inf')\n","best_valid_acc = 0\n","no_improvement_epochs_phase1 = 0\n","patience_phase1 = 5\n","best_model_state_phase = None\n","\n","print(f'Epoch\\tTrain loss\\tValid loss\\tAccuracy')\n","for epoch in range(10):\n","    train_loss = train(train_dataloader,model,loss_fn,optimizer1)\n","    valid_loss, accuracy = test(valid_dataloader,model,loss_fn)\n","    scheduler1.step()\n","    print(f'{epoch+1:<5}\\t{train_loss:<10.5f}\\t{valid_loss:<10.5f}\\t{accuracy:<8.5f}')\n","\n","    if accuracy > best_valid_acc or (accuracy == best_valid_acc and valid_loss < best_valid_loss):\n","        best_valid_acc = accuracy\n","        best_valid_loss = valid_loss\n","        no_improvement_epochs_phase1 = 0\n","        best_model_state_phase = model.state_dict()\n","    else:\n","        no_improvement_epochs_phase1 += 1\n","\n","    if no_improvement_epochs_phase1 >= patience_phase1:\n","        print(\"Early stopping Phase 1\")\n","        break\n","\n","if best_model_state_phase is not None:\n","    model.load_state_dict(best_model_state_phase)\n","    recompute_batch_statistics(model, train_dataloader)\n","\n","optimizer2 = optim.SGD(params=model.parameters(), lr=optimizer1.param_groups[0]['lr'], momentum=0.9)\n","scheduler2 = ReduceLROnPlateau(optimizer2, mode='min', patience=4, factor=0.1)\n","no_improvement_epochs_phase2 = 0\n","patience_phase2 = 5\n","\n","print(f'\\nEpoch\\tTrain loss\\tValid loss\\tAccuracy')\n","for epoch in range(10):\n","    train_loss = train(train_dataloader,model,loss_fn,optimizer2)\n","    valid_loss, accuracy = test(valid_dataloader,model,loss_fn)\n","    scheduler2.step(valid_loss)\n","    print(f'{epoch+1:<5}\\t{train_loss:<10.5f}\\t{valid_loss:<10.5f}\\t{accuracy:<8.5f}')\n","\n","    if accuracy > best_valid_acc or (accuracy == best_valid_acc and valid_loss < best_valid_loss):\n","        best_valid_acc = accuracy\n","        best_valid_loss = valid_loss\n","        no_improvement_epochs_phase2 = 0\n","        best_model_state_phase = model.state_dict()\n","    else:\n","        no_improvement_epochs_phase2 += 1\n","\n","    if no_improvement_epochs_phase2 >= patience_phase2:\n","        print(\"Early stopping Phase 2\")\n","        break\n","\n","if best_model_state_phase is not None:\n","    model.load_state_dict(best_model_state_phase)\n","    recompute_batch_statistics(model, train_dataloader)\n","\n","optimizer3 = optim.AdamW(params=model.parameters(), lr=optimizer2.param_groups[0]['lr'])\n","scheduler3 = ReduceLROnPlateau(optimizer3, mode='min', patience=4, factor=0.1)\n","no_improvement_epochs_phase3 = 0\n","patience_phase3 = 5\n","\n","print(f'\\nEpoch\\tTrain loss\\tValid loss\\tAccuracy')\n","for epoch in range(10):\n","    train_loss = train(train_dataloader,model,loss_fn,optimizer3)\n","    valid_loss, accuracy = test(valid_dataloader,model,loss_fn)\n","    scheduler3.step(valid_loss)\n","    print(f'{epoch+1:<5}\\t{train_loss:<10.5f}\\t{valid_loss:<10.5f}\\t{accuracy:<8.5f}')\n","\n","    if accuracy > best_valid_acc or (accuracy == best_valid_acc and valid_loss < best_valid_loss):\n","        best_valid_acc = accuracy\n","        best_valid_loss = valid_loss\n","        no_improvement_epochs_phase3 = 0\n","        best_model_state_phase3 = model.state_dict()\n","    else:\n","        no_improvement_epochs_phase3 += 1\n","\n","    if no_improvement_epochs_phase3 >= patience_phase3:\n","        print(\"Early stopping Phase 3\")\n","        break\n","\n","if best_model_state_phase is not None:\n","    model.load_state_dict(best_model_state_phase)\n","    recompute_batch_statistics(model, train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LHHnbM181edC","outputId":"90d01d8e-595c-461d-feab-f718c7b5eaca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch\tTrain loss\tValid loss\tAccuracy\n"]}]},{"cell_type":"code","source":["def final_test(dataloader,model,loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    loss, accuracy = 0, 0\n","    predictions = []\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            y_pred = model(X)\n","            loss += loss_fn(y_pred, y).item()\n","            accuracy += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n","            predictions.append(torch.sigmoid(y_pred))\n","    loss /= num_batches\n","    accuracy /= size\n","    return loss, accuracy, predictions\n","\n","\n","test_loss, accuracy, predictions = final_test(test_dataloader,model,loss_fn)\n","\n","ans = [a.argmax().item() for a in predictions]\n","\n","labels = ['adenocarcinoma', 'large carcinoma', 'normal', 'squamous carcinoma']\n","mapping = {0:'adenocarcinoma', 1:'large carcinoma', 2:'normal', 3:'squamous carcinoma'}\n","\n","cm = confusion_matrix([y.item() for _, y in test_dataloader], ans, labels=range(len(labels)))\n","ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(xticks_rotation='vertical')\n","plt.title(f'Accuracy:{cm.diagonal().sum()/cm.sum()*100:.2f}%')\n","plt.show()\n","\n","torch.save(model.state_dict(), f'ct_scan_model({round(cm.diagonal().sum()/cm.sum()*10000)}).pth')"],"metadata":{"id":"3meIzjVz1hpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model1 = CustomResNet(4)\n","model1.load_state_dict(torch.load('ct_scan_model(9016).pth'))\n","\n","model2 = CustomResNet(4)\n","model2.load_state_dict(torch.load('ct_scan_model(9270).pth'))\n","\n","model3 = CustomResNet(4)\n","model3.load_state_dict(torch.load('ct_scan_model(9333).pth'))\n","\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"gytC3x1F1kHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def final_test(dataloader,model,loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    loss, accuracy = 0, 0\n","    predictions = []\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            y_pred = model(X)\n","            loss += loss_fn(y_pred, y).item()\n","            accuracy += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n","            predictions.append(torch.sigmoid(y_pred))\n","    loss /= num_batches\n","    accuracy /= size\n","    return loss, accuracy, predictions\n","\n","averaged_models = [AveragingModel([model1,model2]), AveragingModel([model1,model3]), AveragingModel([model2,model3]), AveragingModel([model1,model2,model3])]\n","modelnames = ['Model 1 + 2', 'Model 1 + 3', 'Model 2 + 3', 'Model 1 + 2 + 3']\n","\n","test_loss, accuracy, predictions1 = final_test(test_dataloader,model1,loss_fn)\n","test_loss, accuracy, predictions2 = final_test(test_dataloader,model2,loss_fn)\n","test_loss, accuracy, predictions3 = final_test(test_dataloader,model3,loss_fn)\n","\n","ans1 = [a.argmax().item() for a in predictions1]\n","ans2 = [a.argmax().item() for a in predictions2]\n","ans3 = [a.argmax().item() for a in predictions3]\n","\n","labels = ['adenocarcinoma', 'large carcinoma', 'normal', 'squamous carcinoma']\n","mapping = {0:'adenocarcinoma', 1:'large carcinoma', 2:'normal', 3:'squamous carcinoma'}\n","\n","cm = confusion_matrix([y.item() for _, y in test_dataloader], ans1, labels=range(len(labels)))\n","ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(xticks_rotation='vertical')\n","cm.diagonal().sum()\n","plt.title(f'Model 1\\nAccuracy:{cm.diagonal().sum()/cm.sum()*100:.2f}%')\n","plt.show()\n","\n","cm = confusion_matrix([y.item() for _, y in test_dataloader], ans2, labels=range(len(labels)))\n","ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(xticks_rotation='vertical')\n","plt.title(f'Model 2\\nAccuracy:{cm.diagonal().sum()/cm.sum()*100:.2f}%')\n","plt.show()\n","\n","cm = confusion_matrix([y.item() for _, y in test_dataloader], ans3, labels=range(len(labels)))\n","ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(xticks_rotation='vertical')\n","plt.title(f'Model 3\\nAccuracy:{cm.diagonal().sum()/cm.sum()*100:.2f}%')\n","plt.show()\n","\n","\n","for modelname, averaged_model in zip(modelnames,averaged_models):\n","    test_loss, accuracy, predictions = final_test(test_dataloader,averaged_model,loss_fn)\n","    ans = [a.argmax().item() for a in predictions]\n","    cm = confusion_matrix([y.item() for _, y in test_dataloader], ans, labels=range(len(labels)))\n","    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(xticks_rotation='vertical')\n","    plt.title(f'{modelname}\\nAccuracy:{cm.diagonal().sum()/cm.sum()*100:.2f}%')\n","    plt.show()\n","\n","# torch.save(model.state_dict(), 'ct_scan_model.pth')"],"metadata":{"id":"fqAv-Fc11mNC"},"execution_count":null,"outputs":[]}]}