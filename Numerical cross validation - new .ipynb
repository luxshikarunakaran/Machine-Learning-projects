{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GqpEknZVcz0o"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score\n","from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut, KFold, cross_val_score\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from sklearn.linear_model import LinearRegression\n","import time\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.applications import MobileNet\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load and preprocess data\n","df = pd.read_csv(\"/content/lung cancer.csv\")\n","df['GENDER'] = df['GENDER'].replace({'M':0, 'F':1})\n","df['LUNG_CANCER'] = df['LUNG_CANCER'].replace({'YES':1, 'NO':0})\n","x = df.drop('LUNG_CANCER', axis=1)\n","y = df['LUNG_CANCER']\n","\n","# Verify dataset shape\n","print(f\"Dataset shape: {df.shape}\")\n","print(f\"Features shape: {x.shape}\")\n","\n","# Split data\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","print(f\"x_train: {x_train.shape}, x_test: {x_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")\n","\n","# Scale features\n","scaler = StandardScaler()\n","x_train_scaled = scaler.fit_transform(x_train)\n","x_test_scaled = scaler.transform(x_test)\n","print(f\"x_train_scaled: {x_train_scaled.shape}, x_test_scaled: {x_test_scaled.shape}\")\n","\n","# Prepare data for CNN/MobileNet\n","n_features = x_train_scaled.shape[1]\n","if n_features != 16:\n","    print(f\"Warning: Expected 16 features, got {n_features}. Adjusting reshape.\")\n","    # Pad or truncate to 16 features if necessary\n","    if n_features < 16:\n","        x_train_scaled = np.pad(x_train_scaled, ((0, 0), (0, 16 - n_features)), mode='constant')\n","        x_test_scaled = np.pad(x_test_scaled, ((0, 0), (0, 16 - n_features)), mode='constant')\n","    else:\n","        x_train_scaled = x_train_scaled[:, :16]\n","        x_test_scaled = x_test_scaled[:, :16]\n","    n_features = 16\n","\n","# Reshape for CNN: 16 features -> 4x4x1\n","x_train_cnn = x_train_scaled.reshape(-1, 4, 4, 1)\n","x_test_cnn = x_test_scaled.reshape(-1, 4, 4, 1)\n","print(f\"x_train_cnn: {x_train_cnn.shape}, x_test_cnn: {x_test_cnn.shape}\")\n","\n","# Reshape for MobileNet: Upscale to 224x224x3\n","x_train_mn = np.repeat(x_train_cnn, 56, axis=1)  # 4 -> 224\n","x_train_mn = np.repeat(x_train_mn, 56, axis=2)\n","x_train_mn = np.repeat(x_train_mn, 3, axis=3)  # 1 -> 3 channels\n","x_test_mn = np.repeat(x_test_cnn, 56, axis=1)\n","x_test_mn = np.repeat(x_test_mn, 56, axis=2)\n","x_test_mn = np.repeat(x_test_mn, 3, axis=3)\n","print(f\"x_train_mn: {x_train_mn.shape}, x_test_mn: {x_test_mn.shape}\")\n","\n","np.random.seed(32)\n","\n","# Initialize lists\n","models = ['GaussianNB', 'SVC', 'Logistic Regression', 'Decision Tree',\n","          'Random Forest', 'Gradient Boosting', 'XGBoost', 'Linear Regression', 'CNN', 'MobileNet']\n","test_accuracies = []\n","train_accuracies = []\n","skfold_means = []\n","loocv_means = []\n","kf_means = []\n","training_times = []\n","y_pred_list = []\n","\n","# CNN training function\n","def train_cnn(x_train, y_train, x_test, y_test):\n","    model = Sequential([\n","        Conv2D(16, (2, 2), activation='relu', input_shape=(4, 4, 1)),\n","        MaxPooling2D((2, 2)),\n","        Flatten(),\n","        Dense(32, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    start_time = time.time()\n","    history = model.fit(x_train, y_train, epochs=20, batch_size=16, validation_data=(x_test, y_test), verbose=0)\n","    training_time = time.time() - start_time\n","    train_acc = history.history['accuracy'][-1]\n","    test_acc = model.evaluate(x_test, y_test)[1]\n","    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n","    return model, test_acc, train_acc, training_time, y_pred\n","\n","# MobileNet training function\n","def train_mobilenet(x_train, y_train, x_test, y_test):\n","    base_model = MobileNet(weights=None, include_top=False, input_shape=(224, 224, 3))\n","    model = Sequential([\n","        base_model,\n","        Flatten(),\n","        Dense(64, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    start_time = time.time()\n","    history = model.fit(x_train, y_train, epochs=10, batch_size=16, validation_data=(x_test, y_test), verbose=0)\n","    training_time = time.time() - start_time\n","    train_acc = history.history['accuracy'][-1]\n","    test_acc = model.evaluate(x_test, y_test)[1]\n","    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n","    return model, test_acc, train_acc, training_time, y_pred\n","\n","# Train models\n","# GaussianNB\n","model_g = GaussianNB()\n","start_time = time.time()\n","model_g.fit(x_train_scaled, y_train)\n","training_times.append(time.time() - start_time)\n","train_accuracies.append(model_g.score(x_train_scaled, y_train))\n","test_accuracies.append(model_g.score(x_test_scaled, y_test))\n","y_pred = model_g.predict(x_test_scaled)\n","y_pred_list.append(y_pred)\n","skf = StratifiedKFold(n_splits=5)\n","skfold_score_g = cross_val_score(model_g, x, y, cv=skf)\n","skfold_means.append(skfold_score_g.mean())\n","loocv = LeaveOneOut()\n","loocv_score_g = cross_val_score(model_g, x, y, cv=loocv)\n","loocv_means.append(loocv_score_g.mean())\n","kf = KFold(n_splits=5)\n","k_fold_score_g = cross_val_score(model_g, x, y, cv=kf)\n","kf_means.append(k_fold_score_g.mean())\n","print(f\"GaussianNB: Train Acc: {train_accuracies[-1]:.4f}, Test Acc: {test_accuracies[-1]:.4f}, Time: {training_times[-1]:.2f}s\")\n","\n","# SVC\n","model2 = SVC(probability=True)\n","start_time = time.time()\n","model2.fit(x_train_scaled, y_train)\n","training_times.append(time.time() - start_time)\n","train_accuracies.append(model2.score(x_train_scaled, y_train))\n","test_accuracies.append(model2.score(x_test_scaled, y_test))\n","y_pred2 = model2.predict(x_test_scaled)\n","y_pred_list.append(y_pred2)\n","skf = StratifiedKFold(n_splits=5)\n","skfold_score = cross_val_score(model2, x, y, cv=skf)\n","skfold_means.append(skfold_score.mean())\n","loocv = LeaveOneOut()\n","loocv_score = cross_val_score(model2, x, y, cv=loocv)\n","loocv_means.append(loocv_score.mean())\n","kf = KFold(n_splits=5)\n","score = cross_val_score(model2, x, y, cv=kf)\n","kf_means.append(score.mean())\n","print(f\"SVC: Train Acc: {train_accuracies[-1]:.4f}, Test Acc: {test_accuracies[-1]:.4f}, Time: {training_times[-1]:.2f}s\")\n","\n","# Logistic Regression\n","model_lr = LogisticRegression()\n","start_time = time.time()\n","model_lr.fit(x_train_scaled, y_train)\n","training_times.append(time.time() - start_time)\n","train_accuracies.append(model_lr.score(x_train_scaled, y_train))\n","test_accuracies.append(model_lr.score(x_test_scaled, y_test))\n","y_pred_lr = model_lr.predict(x_test_scaled)\n","y_pred_list.append(y_pred_lr)\n","skf = StratifiedKFold(n_splits=5)\n","skfold_score_lr = cross_val_score(model_lr, x, y, cv=skf)\n","skfold_means.append(skfold_score_lr.mean())\n","loocv = LeaveOneOut()\n","loocv_score_lr = cross_val_score(model_lr, x, y, cv=loocv)\n","loocv_means.append(loocv_score_lr.mean())\n","kf = KFold(n_splits=5)\n","kf_score_lr = cross_val_score(model_lr, x, y, cv=kf)\n","kf_means.append(kf_score_lr.mean())\n","print(f\"Logistic Regression: Train Acc: {train_accuracies[-1]:.4f}, Test Acc: {test_accuracies[-1]:.4f}, Time: {training_times[-1]:.2f}s\")\n","\n","# Decision Tree\n","model_dt = DecisionTreeClassifier()\n","start_time = time.time()\n","model_dt.fit(x_train_scaled, y_train)\n","training_times.append(time.time() - start_time)\n","train_accuracies.append(model_dt.score(x_train_scaled, y_train))\n","test_accuracies.append(model_dt.score(x_test_scaled, y_test))\n","y_pred_dt = model_dt.predict(x_test_scaled)\n","y_pred_list.append(y_pred_dt)\n","skf = StratifiedKFold(n_splits=5)\n","skfold_score_dt = cross_val_score(model_dt, x, y, cv=skf)\n","skfold_means.append(skfold_score_dt.mean())\n","loocv = LeaveOneOut()\n","loocv_score_dt = cross_val_score(model_dt, x, y, cv=loocv)\n","loocv_means.append(loocv_score_dt.mean())\n","kf = KFold(n_splits=5)\n","kf_score_dt = cross_val_score(model_dt, x, y, cv=kf)\n","kf_means.append(kf_score_dt.mean())\n","print(f\"Decision Tree: Train Acc: {train_accuracies[-1]:.4f}, Test Acc: {test_accuracies[-1]:.4f}, Time: {training_times[-1]:.2f}s\")\n","\n","# Random Forest\n","model_rf = RandomForestClassifier()\n","start_time = time.time()\n","model_rf.fit(x_train_scaled, y_train)\n","training_times.append(time.time() - start_time)\n","train_accuracies.append(model_rf.score(x_train_scaled, y_train))\n","test_accuracies.append(model_rf.score(x_test_scaled, y_test))\n","y_pred_rf = model_rf.predict(x_test_scaled)\n","y_pred_list.append(y_pred_rf)\n","skf = StratifiedKFold(n_splits=5)\n","skfold_score_rf = cross_val_score(model_rf, x, y, cv=skf)\n","skfold_means.append(skfold_score_rf.mean())\n","loocv = LeaveOneOut()\n","loocv_score_rf = cross_val_score(model_rf, x, y, cv=loocv)\n","loocv_means.append(loocv_score_rf.mean())\n","kf = KFold(n_splits=5)\n","kf_score_rf = cross_val_score(model_rf, x, y, cv=kf)\n","kf_means.append(kf_score_rf.mean())\n","print(f\"Random Forest: Train Acc: {train_accuracies[-1]:.4f}, Test Acc: {test_accuracies[-1]:.4f}, Time: {training_times[-1]:.2f}s\")\n","\n","# Gradient Boosting\n","model_gb = GradientBoostingClassifier()\n","start_time = time.time()\n","model_gb.fit(x_train_scaled, y_train)\n","training_times.append(time.time() - start_time)\n","train_accuracies.append(model_gb.score(x_train_scaled, y_train))\n","test_accuracies.append(model_gb.score(x_test_scaled, y_test))\n","y_pred_gb = model_gb.predict(x_test_scaled)\n","y_pred_list.append(y_pred_gb)\n","skf = StratifiedKFold(n_splits=5)\n","skfold_score_gb = cross_val_score(model_gb, x, y, cv=skf)\n","skfold_means.append(skfold_score_gb.mean())\n","loocv = LeaveOneOut()\n","loocv_score_gb = cross_val_score(model_gb, x, y, cv=loocv)\n","loocv_means.append(loocv_score_gb.mean())\n","kf = KFold(n_splits=5)\n","kf_score_gb = cross_val_score(model_gb, x, y, cv=kf)\n","kf_means.append(kf_score_gb.mean())\n","print(f\"Gradient Boosting: Train Acc: {train_accuracies[-1]:.4f}, Test Acc: {test_accuracies[-1]:.4f}, Time: {training_times[-1]:.2f}s\")\n","\n","# XGBoost\n","model_xgb = XGBClassifier()\n","start_time = time.time()\n","model_xgb.fit(x_train_scaled, y_train)\n","training_times.append(time.time() - start_time)\n","train_accuracies.append(model_xgb.score(x_train_scaled, y_train))\n","test_accuracies.append(model_xgb.score(x_test_scaled, y_test))\n","y_pred_xgb = model_xgb.predict(x_test_scaled)\n","y_pred_list.append(y_pred_xgb)\n","skf = StratifiedKFold(n_splits=5)\n","skfold_score_xgb = cross_val_score(model_xgb, x, y, cv=skf)\n","skfold_means.append(skfold_score_xgb.mean())\n","loocv = LeaveOneOut()\n","loocv_score_xgb = cross_val_score(model_xgb, x, y, cv=loocv)\n","loocv_means.append(loocv_score_xgb.mean())\n","kf = KFold(n_splits=5)\n","kf_score_xgb = cross_val_score(model_xgb, x, y, cv=kf)\n","kf_means.append(kf_score_xgb.mean())\n","print(f\"XGBoost: Train Acc: {train_accuracies[-1]:.4f}, Test Acc: {test_accuracies[-1]:.4f}, Time: {training_times[-1]:.2f}s\")\n","\n","# Linear Regression (as classifier)\n","model_lre = LinearRegression()\n","start_time = time.time()\n","model_lre.fit(x_train_scaled, y_train)\n","training_times.append(time.time() - start_time)\n","train_pred_lre = (model_lre.predict(x_train_scaled) > 0.5).astype(int)\n","test_pred_lre = (model_lre.predict(x_test_scaled) > 0.5).astype(int)\n","train_accuracies.append(accuracy_score(y_train, train_pred_lre))\n","test_accuracies.append(accuracy_score(y_test, test_pred_lre))\n","y_pred_lre = test_pred_lre\n","y_pred_list.append(y_pred_lre)\n","num_bins = 5\n","y_binned = np.digitize(y, bins=np.linspace(min(y), max(y), num_bins))\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","skfold_score_lre = cross_val_score(model_lre, x, y_binned, cv=skf, scoring='r2')\n","skfold_means.append(skfold_score_lre.mean())\n","loocv = LeaveOneOut()\n","loocv_score_lre = cross_val_score(model_lre, x, y, cv=loocv)\n","loocv_means.append(loocv_score_lre.mean())\n","kf = KFold(n_splits=5)\n","kf_score_lre = cross_val_score(model_lre, x, y, cv=kf, scoring='r2')\n","kf_means.append(kf_score_lre.mean())\n","print(f\"Linear Regression: Train Acc: {train_accuracies[-1]:.4f}, Test Acc: {test_accuracies[-1]:.4f}, Time: {training_times[-1]:.2f}s\")\n","\n","# CNN\n","model_cnn, test_acc_cnn, train_acc_cnn, time_cnn, y_pred_cnn = train_cnn(x_train_cnn, y_train, x_test_cnn, y_test)\n","training_times.append(time_cnn)\n","train_accuracies.append(train_acc_cnn)\n","test_accuracies.append(test_acc_cnn)\n","y_pred_list.append(y_pred_cnn)\n","skfold_means.append(0)\n","loocv_means.append(0)\n","kf_means.append(0)\n","print(f\"CNN: Train Acc: {train_acc_cnn:.4f}, Test Acc: {test_acc_cnn:.4f}, Time: {time_cnn:.2f}s\")\n","\n","# MobileNet\n","model_mn, test_acc_mn, train_acc_mn, time_mn, y_pred_mn = train_mobilenet(x_train_mn, y_train, x_test_mn, y_test)\n","training_times.append(time_mn)\n","train_accuracies.append(train_acc_mn)\n","test_accuracies.append(test_acc_mn)\n","y_pred_list.append(y_pred_mn)\n","skfold_means.append(0)\n","loocv_means.append(0)\n","kf_means.append(0)\n","print(f\"MobileNet: Train Acc: {train_acc_mn:.4f}, Test Acc: {test_acc_mn:.4f}, Time: {time_mn:.2f}s\")\n","\n","# Plotting Training Times\n","plt.figure(figsize=(12, 6))\n","sns.barplot(x=models, y=training_times)\n","plt.title('Training Times of Different Models')\n","plt.ylabel('Time (seconds)')\n","plt.xticks(rotation=45)\n","plt.show()\n","\n","# Plotting Training and Testing Accuracies\n","x = np.arange(len(models))\n","width = 0.35\n","fig, ax = plt.subplots(figsize=(14, 6))\n","rects1 = ax.bar(x - width/2, train_accuracies, width, label='Training Accuracy')\n","rects2 = ax.bar(x + width/2, test_accuracies, width, label='Testing Accuracy')\n","ax.set_ylabel('Accuracy')\n","ax.set_title('Training and Testing Accuracies of Different Models')\n","ax.set_xticks(x)\n","ax.set_xticklabels(models)\n","ax.legend()\n","plt.xticks(rotation=45)\n","plt.ylim(0, 1)\n","plt.show()\n","\n","# Plotting Test Accuracies\n","plt.figure(figsize=(12, 6))\n","sns.barplot(x=models, y=test_accuracies)\n","plt.title('Test Accuracies of Different Models')\n","plt.ylabel('Accuracy')\n","plt.xticks(rotation=45)\n","plt.ylim(0, 1)\n","plt.show()\n","\n","# Plotting Validation Scores\n","fig, ax = plt.subplots(figsize=(12, 6))\n","rects1 = ax.bar(x - width, skfold_means, width, label='Stratified K-Fold')\n","rects2 = ax.bar(x, loocv_means, width, label='Leave-One-Out')\n","rects3 = ax.bar(x + width, kf_means, width, label='K-Fold')\n","ax.set_ylabel('Scores')\n","ax.set_title('Validation Scores of Different Models')\n","ax.set_xticks(x)\n","ax.set_xticklabels(models)\n","ax.legend()\n","plt.xticks(rotation=45)\n","plt.show()\n","\n","# ROC Curve\n","plt.figure(figsize=(12, 6))\n","model_instances = [model_g, model2, model_lr, model_dt, model_rf, model_gb, model_xgb, model_lre, model_cnn, model_mn]\n","for model, y_pred, name in zip(model_instances, y_pred_list, models):\n","    if name in ['CNN', 'MobileNet']:\n","        fpr, tpr, _ = roc_curve(y_test, y_pred)\n","    elif name == 'Linear Regression':\n","        fpr, tpr, _ = roc_curve(y_test, model.predict(x_test_scaled))\n","    else:\n","        fpr, tpr, _ = roc_curve(y_test, model.predict_proba(x_test_scaled)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(x_test_scaled))\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","# Results Table\n","results = []\n","def specificity(confusion):\n","    TN, FP, FN, TP = confusion.ravel()\n","    return TN / (TN + FP)\n","for model, y_pred, skfold_score, loocv_score, kf_score, name in zip(\n","    model_instances, y_pred_list,\n","    [skfold_score_g, skfold_score, skfold_score_lr, skfold_score_dt, skfold_score_rf, skfold_score_gb, skfold_score_xgb, skfold_score_lre, 0, 0],\n","    [loocv_score_g, loocv_score, loocv_score_lr, loocv_score_dt, loocv_score_rf, loocv_score_gb, loocv_score_xgb, loocv_score_lre, 0, 0],\n","    [k_fold_score_g, score, kf_score_lr, kf_score_dt, kf_score_rf, kf_score_gb, kf_score_xgb, kf_score_lre, 0, 0],\n","    models\n","):\n","    acc = test_accuracies[models.index(name)]\n","    results.append({\n","        'Model': name,\n","        'Testing Accuracy': acc,\n","        'K-Fold': kf_score.mean() if name not in ['CNN', 'MobileNet'] else 0,\n","        'Stratified K-Fold': skfold_score.mean() if name not in ['CNN', 'MobileNet'] else 0,\n","        'Leave-One-Out': loocv_score.mean() if name not in ['CNN', 'MobileNet'] else 0\n","    })\n","results_df = pd.DataFrame(results)\n","best_model = results_df.loc[results_df['Testing Accuracy'].idxmax()]\n","print(\"\\nComparison Table:\")\n","print(results_df)\n","print(\"\\nBest Model:\")\n","print(best_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":563},"executionInfo":{"elapsed":5190,"status":"error","timestamp":1744394761261,"user":{"displayName":"luxshi karunakaran","userId":"14981122548573112690"},"user_tz":-330},"id":"YBZPuiO9BTId","outputId":"2b8f986e-0542-4ff3-be74-365d31f109cf"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-2-e0922611fc16>:23: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df['GENDER'] = df['GENDER'].replace({'M':0, 'F':1})\n","<ipython-input-2-e0922611fc16>:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df['LUNG_CANCER'] = df['LUNG_CANCER'].replace({'YES':1, 'NO':0})\n"]},{"name":"stdout","output_type":"stream","text":["Dataset shape: (5871, 16)\n","Features shape: (5871, 15)\n","x_train: (4696, 15), x_test: (1175, 15), y_train: (4696,), y_test: (1175,)\n","x_train_scaled: (4696, 15), x_test_scaled: (1175, 15)\n","Warning: Expected 16 features, got 15. Adjusting reshape.\n","x_train_cnn: (4696, 4, 4, 1), x_test_cnn: (1175, 4, 4, 1)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-e0922611fc16>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mx_train_mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 4 -> 224\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mx_train_mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mx_train_mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1 -> 3 channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mx_test_mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mx_test_mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(a, repeats, axis)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \"\"\"\n\u001b[0;32m--> 480\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'repeat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score\n","from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut, KFold, cross_val_score\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from sklearn.linear_model import LinearRegression\n","import time\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.applications import MobileNet\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load and preprocess data\n","df = pd.read_csv(\"/content/lung cancer.csv\")\n","df['GENDER'] = df['GENDER'].replace({'M':0, 'F':1})\n","df['LUNG_CANCER'] = df['LUNG_CANCER'].replace({'YES':1, 'NO':0})\n","x = df.drop('LUNG_CANCER', axis=1)\n","y = df['LUNG_CANCER']\n","\n","# Verify dataset shape\n","print(f\"Dataset shape: {df.shape}\")\n","print(f\"Features shape: {x.shape}\")\n","\n","# Split data\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","print(f\"x_train: {x_train.shape}, x_test: {x_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")\n","\n","# Scale features\n","scaler = StandardScaler()\n","x_train_scaled = scaler.fit_transform(x_train)\n","x_test_scaled = scaler.transform(x_test)\n","print(f\"x_train_scaled: {x_train_scaled.shape}, x_test_scaled: {x_test_scaled.shape}\")\n","\n","# Prepare data for CNN/MobileNet\n","n_features = x_train_scaled.shape[1]\n","if n_features != 16:\n","    print(f\"Warning: Expected 16 features, got {n_features}. Adjusting reshape.\")\n","    # Pad or truncate to 16 features if necessary\n","    if n_features < 16:\n","        x_train_scaled = np.pad(x_train_scaled, ((0, 0), (0, 16 - n_features)), mode='constant')\n","        x_test_scaled = np.pad(x_test_scaled, ((0, 0), (0, 16 - n_features)), mode='constant')\n","    else:\n","        x_train_scaled = x_train_scaled[:, :16]\n","        x_test_scaled = x_test_scaled[:, :16]\n","    n_features = 16\n","\n","# Reshape for CNN: 16 features -> 4x4x1\n","x_train_cnn = x_train_scaled.reshape(-1, 4, 4, 1)\n","x_test_cnn = x_test_scaled.reshape(-1, 4, 4, 1)\n","print(f\"x_train_cnn: {x_train_cnn.shape}, x_test_cnn: {x_test_cnn.shape}\")\n","\n","# Reshape for MobileNet: Upscale to 224x224x3\n","x_train_mn = np.repeat(x_train_cnn, 56, axis=1)  # 4 -> 224\n","x_train_mn = np.repeat(x_train_mn, 56, axis=2)\n","x_train_mn = np.repeat(x_train_mn, 3, axis=3)  # 1 -> 3 channels\n","x_test_mn = np.repeat(x_test_cnn, 56, axis=1)\n","x_test_mn = np.repeat(x_test_mn, 56, axis=2)\n","x_test_mn = np.repeat(x_test_mn, 3, axis=3)\n","print(f\"x_train_mn: {x_train_mn.shape}, x_test_mn: {x_test_mn.shape}\")\n","\n","np.random.seed(32)\n","\n","# Initialize lists\n","models = ['GaussianNB', 'SVC', 'Logistic Regression', 'Decision Tree',\n","          'Random Forest', 'Gradient Boosting', 'XGBoost', 'Linear Regression', 'CNN', 'MobileNet']\n","test_accuracies = []\n","train_accuracies = []\n","skfold_means = []\n","loocv_means = []\n","kf_means = []\n","holdout_times = []\n","skfold_times = []\n","loocv_times = []\n","kf_times = []\n","y_pred_list = []\n","\n","# CNN training function\n","def train_cnn(x_train, y_train, x_test, y_test):\n","    model = Sequential([\n","        Conv2D(16, (2, 2), activation='relu', input_shape=(4, 4, 1)),\n","        MaxPooling2D((2, 2)),\n","        Flatten(),\n","        Dense(32, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    start_time = time.time()\n","    history = model.fit(x_train, y_train, epochs=20, batch_size=16, validation_data=(x_test, y_test), verbose=0)\n","    training_time = time.time() - start_time\n","    train_acc = history.history['accuracy'][-1]\n","    test_acc = model.evaluate(x_test, y_test)[1]\n","    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n","    return model, test_acc, train_acc, training_time, y_pred\n","\n","# MobileNet training function\n","def train_mobilenet(x_train, y_train, x_test, y_test):\n","    base_model = MobileNet(weights=None, include_top=False, input_shape=(224, 224, 3))\n","    model = Sequential([\n","        base_model,\n","        Flatten(),\n","        Dense(64, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    start_time = time.time()\n","    history = model.fit(x_train, y_train, epochs=10, batch_size=16, validation_data=(x_test, y_test), verbose=0)\n","    training_time = time.time() - start_time\n","    train_acc = history.history['accuracy'][-1]\n","    test_acc = model.evaluate(x_test, y_test)[1]\n","    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n","    return model, test_acc, train_acc, training_time, y_pred\n","\n","# Function to measure cross-validation time\n","def measure_cv_time(model, x, y, cv_method, cv_params):\n","    start_time = time.time()\n","    scores = cross_val_score(model, x, y, cv=cv_method(**cv_params))\n","    return time.time() - start_time, scores.mean()\n","\n","# Train models and measure times\n","for model_name in models:\n","    print(f\"\\n=== Training {model_name} ===\")\n","\n","    # Initialize model\n","    if model_name == 'GaussianNB':\n","        model = GaussianNB()\n","    elif model_name == 'SVC':\n","        model = SVC(probability=True)\n","    elif model_name == 'Logistic Regression':\n","        model = LogisticRegression()\n","    elif model_name == 'Decision Tree':\n","        model = DecisionTreeClassifier()\n","    elif model_name == 'Random Forest':\n","        model = RandomForestClassifier()\n","    elif model_name == 'Gradient Boosting':\n","        model = GradientBoostingClassifier()\n","    elif model_name == 'XGBoost':\n","        model = XGBClassifier()\n","    elif model_name == 'Linear Regression':\n","        model = LinearRegression()\n","    elif model_name == 'CNN':\n","        pass  # Handled separately\n","    elif model_name == 'MobileNet':\n","        pass  # Handled separately\n","\n","    # Hold-out method timing\n","    if model_name not in ['CNN', 'MobileNet']:\n","        start_time = time.time()\n","        model.fit(x_train_scaled, y_train)\n","        holdout_time = time.time() - start_time\n","        holdout_times.append(holdout_time)\n","\n","        # Predictions\n","        train_acc = model.score(x_train_scaled, y_train)\n","        test_acc = model.score(x_test_scaled, y_test)\n","        y_pred = model.predict(x_test_scaled)\n","    else:\n","        if model_name == 'CNN':\n","            model, test_acc, train_acc, holdout_time, y_pred = train_cnn(x_train_cnn, y_train, x_test_cnn, y_test)\n","        else:\n","            model, test_acc, train_acc, holdout_time, y_pred = train_mobilenet(x_train_mn, y_train, x_test_mn, y_test)\n","        holdout_times.append(holdout_time)\n","\n","    train_accuracies.append(train_acc)\n","    test_accuracies.append(test_acc)\n","    y_pred_list.append(y_pred)\n","\n","    # Cross-validation timing (skip for CNN/MobileNet)\n","    if model_name not in ['CNN', 'MobileNet']:\n","        # K-Fold\n","        kf_time, kf_mean = measure_cv_time(model, x, y, KFold, {'n_splits': 5})\n","        kf_times.append(kf_time)\n","        kf_means.append(kf_mean)\n","\n","        # Stratified K-Fold\n","        skf_time, skf_mean = measure_cv_time(model, x, y, StratifiedKFold, {'n_splits': 5})\n","        skfold_times.append(skf_time)\n","        skfold_means.append(skf_mean)\n","\n","        # Leave-One-Out\n","        loo_time, loo_mean = measure_cv_time(model, x, y, LeaveOneOut, {})\n","        loocv_times.append(loo_time)\n","        loocv_means.append(loo_mean)\n","    else:\n","        kf_times.append(0)\n","        skfold_times.append(0)\n","        loocv_times.append(0)\n","        kf_means.append(0)\n","        skfold_means.append(0)\n","        loocv_means.append(0)\n","\n","    print(f\"Hold-out time: {holdout_time:.4f}s\")\n","    if model_name not in ['CNN', 'MobileNet']:\n","        print(f\"K-Fold time: {kf_time:.4f}s, mean accuracy: {kf_mean:.4f}\")\n","        print(f\"Stratified K-Fold time: {skf_time:.4f}s, mean accuracy: {skf_mean:.4f}\")\n","        print(f\"LOOCV time: {loo_time:.4f}s, mean accuracy: {loo_mean:.4f}\")\n","\n","# Create timing results DataFrame\n","timing_results = pd.DataFrame({\n","    'Model': models,\n","    'Hold-Out Time (s)': holdout_times,\n","    'K-Fold Time (s)': kf_times,\n","    'Stratified K-Fold Time (s)': skfold_times,\n","    'LOOCV Time (s)': loocv_times,\n","    'Hold-Out Test Acc': test_accuracies,\n","    'K-Fold Mean Acc': kf_means,\n","    'Stratified K-Fold Mean Acc': skfold_means,\n","    'LOOCV Mean Acc': loocv_means\n","})\n","\n","print(\"\\n=== Timing Results ===\")\n","print(timing_results)\n","\n","# Plotting Training Times\n","plt.figure(figsize=(14, 6))\n","sns.barplot(x='Model', y='Hold-Out Time (s)', data=timing_results)\n","plt.title('Training Times (Hold-Out Method)')\n","plt.ylabel('Time (seconds)')\n","plt.xticks(rotation=45)\n","plt.show()\n","\n","# Plotting Cross-Validation Times\n","plt.figure(figsize=(14, 6))\n","timing_results_melted = timing_results.melt(id_vars=['Model'],\n","                                          value_vars=['K-Fold Time (s)', 'Stratified K-Fold Time (s)', 'LOOCV Time (s)'],\n","                                          var_name='CV Method', value_name='Time (s)')\n","sns.barplot(x='Model', y='Time (s)', hue='CV Method', data=timing_results_melted)\n","plt.title('Cross-Validation Times by Method')\n","plt.ylabel('Time (seconds)')\n","plt.xticks(rotation=45)\n","plt.legend(title='CV Method')\n","plt.show()\n","\n","# Plotting Accuracy Comparison\n","plt.figure(figsize=(14, 6))\n","accuracy_results_melted = timing_results.melt(id_vars=['Model'],\n","                                            value_vars=['Hold-Out Test Acc', 'K-Fold Mean Acc',\n","                                                       'Stratified K-Fold Mean Acc', 'LOOCV Mean Acc'],\n","                                            var_name='Method', value_name='Accuracy')\n","sns.barplot(x='Model', y='Accuracy', hue='Method', data=accuracy_results_melted)\n","plt.title('Accuracy Comparison Across Methods')\n","plt.ylabel('Accuracy')\n","plt.ylim(0, 1)\n","plt.xticks(rotation=45)\n","plt.legend(title='Evaluation Method')\n","plt.show()\n","\n","# Print comprehensive results\n","print(\"\\n=== Comprehensive Results ===\")\n","print(timing_results.to_string())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"lP49hNvPB-QG","outputId":"248b37c5-0905-4d22-bba0-5db1d05cf4fd"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-3-5739dfa3c9bd>:23: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df['GENDER'] = df['GENDER'].replace({'M':0, 'F':1})\n","<ipython-input-3-5739dfa3c9bd>:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df['LUNG_CANCER'] = df['LUNG_CANCER'].replace({'YES':1, 'NO':0})\n"]},{"name":"stdout","output_type":"stream","text":["Dataset shape: (5871, 16)\n","Features shape: (5871, 15)\n","x_train: (4696, 15), x_test: (1175, 15), y_train: (4696,), y_test: (1175,)\n","x_train_scaled: (4696, 15), x_test_scaled: (1175, 15)\n","Warning: Expected 16 features, got 15. Adjusting reshape.\n","x_train_cnn: (4696, 4, 4, 1), x_test_cnn: (1175, 4, 4, 1)\n","x_train_mn: (4696, 224, 224, 3), x_test_mn: (1175, 224, 224, 3)\n","\n","=== Training GaussianNB ===\n","Hold-out time: 0.0106s\n","K-Fold time: 0.0620s, mean accuracy: 0.9061\n","Stratified K-Fold time: 0.0530s, mean accuracy: 0.9061\n","LOOCV time: 50.1738s, mean accuracy: 0.9061\n","\n","=== Training SVC ===\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-5739dfa3c9bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# Leave-One-Out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mloo_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloo_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure_cv_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeaveOneOut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mloocv_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloo_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mloocv_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloo_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-5739dfa3c9bd>\u001b[0m in \u001b[0;36mmeasure_cv_time\u001b[0;34m(model, x, y, cv_method, cv_params)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeasure_cv_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcv_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score\n","from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut, KFold, cross_val_score\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from sklearn.linear_model import LinearRegression\n","import time\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.applications import MobileNet\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load and preprocess data\n","df = pd.read_csv(\"/content/lung cancer.csv\")\n","df['GENDER'] = df['GENDER'].replace({'M':0, 'F':1})\n","df['LUNG_CANCER'] = df['LUNG_CANCER'].replace({'YES':1, 'NO':0})\n","x = df.drop('LUNG_CANCER', axis=1)\n","y = df['LUNG_CANCER']\n","\n","# Verify dataset shape\n","print(f\"Dataset shape: {df.shape}\")\n","print(f\"Features shape: {x.shape}\")\n","\n","# Split data\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","print(f\"x_train: {x_train.shape}, x_test: {x_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")\n","\n","# Scale features\n","scaler = StandardScaler()\n","x_train_scaled = scaler.fit_transform(x_train)\n","x_test_scaled = scaler.transform(x_test)\n","print(f\"x_train_scaled: {x_train_scaled.shape}, x_test_scaled: {x_test_scaled.shape}\")\n","\n","# Prepare data for CNN/MobileNet\n","n_features = x_train_scaled.shape[1]\n","if n_features != 16:\n","    print(f\"Warning: Expected 16 features, got {n_features}. Adjusting reshape.\")\n","    # Pad or truncate to 16 features if necessary\n","    if n_features < 16:\n","        x_train_scaled = np.pad(x_train_scaled, ((0, 0), (0, 16 - n_features)), mode='constant')\n","        x_test_scaled = np.pad(x_test_scaled, ((0, 0), (0, 16 - n_features)), mode='constant')\n","    else:\n","        x_train_scaled = x_train_scaled[:, :16]\n","        x_test_scaled = x_test_scaled[:, :16]\n","    n_features = 16\n","\n","# Reshape for CNN: 16 features -> 4x4x1\n","x_train_cnn = x_train_scaled.reshape(-1, 4, 4, 1)\n","x_test_cnn = x_test_scaled.reshape(-1, 4, 4, 1)\n","print(f\"x_train_cnn: {x_train_cnn.shape}, x_test_cnn: {x_test_cnn.shape}\")\n","\n","# Reshape for MobileNet: Upscale to 224x224x3\n","x_train_mn = np.repeat(x_train_cnn, 56, axis=1)  # 4 -> 224\n","x_train_mn = np.repeat(x_train_mn, 56, axis=2)\n","x_train_mn = np.repeat(x_train_mn, 3, axis=3)  # 1 -> 3 channels\n","x_test_mn = np.repeat(x_test_cnn, 56, axis=1)\n","x_test_mn = np.repeat(x_test_mn, 56, axis=2)\n","x_test_mn = np.repeat(x_test_mn, 3, axis=3)\n","print(f\"x_train_mn: {x_train_mn.shape}, x_test_mn: {x_test_mn.shape}\")\n","\n","np.random.seed(32)\n","\n","# Initialize lists\n","models = ['GaussianNB', 'SVC', 'Logistic Regression', 'Decision Tree',\n","          'Random Forest', 'Gradient Boosting', 'XGBoost', 'Linear Regression', 'CNN', 'MobileNet']\n","test_accuracies = []\n","train_accuracies = []\n","skfold_means = []\n","loocv_means = []\n","kf_means = []\n","holdout_times = []\n","skfold_times = []\n","loocv_times = []\n","kf_times = []\n","y_pred_list = []\n","\n","# CNN training function\n","def train_cnn(x_train, y_train, x_test, y_test):\n","    model = Sequential([\n","        Conv2D(16, (2, 2), activation='relu', input_shape=(4, 4, 1)),\n","        MaxPooling2D((2, 2)),\n","        Flatten(),\n","        Dense(32, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    start_time = time.time()\n","    history = model.fit(x_train, y_train, epochs=20, batch_size=16, validation_data=(x_test, y_test), verbose=0)\n","    training_time = time.time() - start_time\n","    train_acc = history.history['accuracy'][-1]\n","    test_acc = model.evaluate(x_test, y_test)[1]\n","    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n","    return model, test_acc, train_acc, training_time, y_pred\n","\n","# MobileNet training function\n","def train_mobilenet(x_train, y_train, x_test, y_test):\n","    base_model = MobileNet(weights=None, include_top=False, input_shape=(224, 224, 3))\n","    model = Sequential([\n","        base_model,\n","        Flatten(),\n","        Dense(64, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    start_time = time.time()\n","    history = model.fit(x_train, y_train, epochs=10, batch_size=16, validation_data=(x_test, y_test), verbose=0)\n","    training_time = time.time() - start_time\n","    train_acc = history.history['accuracy'][-1]\n","    test_acc = model.evaluate(x_test, y_test)[1]\n","    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n","    return model, test_acc, train_acc, training_time, y_pred\n","\n","# Function to measure cross-validation time\n","def measure_cv_time(model, x, y, cv_method, cv_params):\n","    start_time = time.time()\n","    scores = cross_val_score(model, x, y, cv=cv_method(**cv_params))\n","    return time.time() - start_time, scores.mean()\n","\n","# Train models and measure times\n","for model_name in models:\n","    print(f\"\\n=== Training {model_name} ===\")\n","\n","    # Initialize model\n","    if model_name == 'GaussianNB':\n","        model = GaussianNB()\n","    elif model_name == 'SVC':\n","        model = SVC(probability=True)\n","    elif model_name == 'Logistic Regression':\n","        model = LogisticRegression()\n","    elif model_name == 'Decision Tree':\n","        model = DecisionTreeClassifier()\n","    elif model_name == 'Random Forest':\n","        model = RandomForestClassifier()\n","    elif model_name == 'Gradient Boosting':\n","        model = GradientBoostingClassifier()\n","    elif model_name == 'XGBoost':\n","        model = XGBClassifier()\n","    elif model_name == 'Linear Regression':\n","        model = LinearRegression()\n","    elif model_name == 'CNN':\n","        pass  # Handled separately\n","    elif model_name == 'MobileNet':\n","        pass  # Handled separately\n","\n","    # Hold-out method timing\n","    if model_name not in ['CNN', 'MobileNet']:\n","        start_time = time.time()\n","        model.fit(x_train_scaled, y_train)\n","        holdout_time = time.time() - start_time\n","        holdout_times.append(holdout_time)\n","\n","        # Predictions\n","        train_acc = model.score(x_train_scaled, y_train)\n","        test_acc = model.score(x_test_scaled, y_test)\n","        y_pred = model.predict(x_test_scaled)\n","    else:\n","        if model_name == 'CNN':\n","            model, test_acc, train_acc, holdout_time, y_pred = train_cnn(x_train_cnn, y_train, x_test_cnn, y_test)\n","        else:\n","            model, test_acc, train_acc, holdout_time, y_pred = train_mobilenet(x_train_mn, y_train, x_test_mn, y_test)\n","        holdout_times.append(holdout_time)\n","\n","    train_accuracies.append(train_acc)\n","    test_accuracies.append(test_acc)\n","    y_pred_list.append(y_pred)\n","\n","    # Cross-validation timing (skip for CNN/MobileNet)\n","    if model_name not in ['CNN', 'MobileNet']:\n","        # K-Fold\n","        kf_time, kf_mean = measure_cv_time(model, x, y, KFold, {'n_splits': 5})\n","        kf_times.append(kf_time)\n","        kf_means.append(kf_mean)\n","\n","        # Stratified K-Fold\n","        skf_time, skf_mean = measure_cv_time(model, x, y, StratifiedKFold, {'n_splits': 5})\n","        skfold_times.append(skf_time)\n","        skfold_means.append(skf_mean)\n","\n","        # Leave-One-Out\n","        loo_time, loo_mean = measure_cv_time(model, x, y, LeaveOneOut, {})\n","        loocv_times.append(loo_time)\n","        loocv_means.append(loo_mean)\n","    else:\n","        kf_times.append(0)\n","        skfold_times.append(0)\n","        loocv_times.append(0)\n","        kf_means.append(0)\n","        skfold_means.append(0)\n","        loocv_means.append(0)\n","\n","    print(f\"Hold-out time: {holdout_time:.4f}s\")\n","    if model_name not in ['CNN', 'MobileNet']:\n","        print(f\"K-Fold time: {kf_time:.4f}s, mean accuracy: {kf_mean:.4f}\")\n","        print(f\"Stratified K-Fold time: {skf_time:.4f}s, mean accuracy: {skf_mean:.4f}\")\n","        print(f\"LOOCV time: {loo_time:.4f}s, mean accuracy: {loo_mean:.4f}\")\n","\n","# Create timing results DataFrame\n","timing_results = pd.DataFrame({\n","    'Model': models,\n","    'Hold-Out Time (s)': holdout_times,\n","    'K-Fold Time (s)': kf_times,\n","    'Stratified K-Fold Time (s)': skfold_times,\n","    'LOOCV Time (s)': loocv_times,\n","    'Hold-Out Test Acc': test_accuracies,\n","    'K-Fold Mean Acc': kf_means,\n","    'Stratified K-Fold Mean Acc': skfold_means,\n","    'LOOCV Mean Acc': loocv_means\n","})\n","\n","print(\"\\n=== Timing Results ===\")\n","print(timing_results)\n","\n","# Plotting Training Times\n","plt.figure(figsize=(14, 6))\n","sns.barplot(x='Model', y='Hold-Out Time (s)', data=timing_results)\n","plt.title('Training Times (Hold-Out Method)')\n","plt.ylabel('Time (seconds)')\n","plt.xticks(rotation=45)\n","plt.show()\n","\n","# Plotting Cross-Validation Times\n","plt.figure(figsize=(14, 6))\n","timing_results_melted = timing_results.melt(id_vars=['Model'],\n","                                          value_vars=['K-Fold Time (s)', 'Stratified K-Fold Time (s)', 'LOOCV Time (s)'],\n","                                          var_name='CV Method', value_name='Time (s)')\n","sns.barplot(x='Model', y='Time (s)', hue='CV Method', data=timing_results_melted)\n","plt.title('Cross-Validation Times by Method')\n","plt.ylabel('Time (seconds)')\n","plt.xticks(rotation=45)\n","plt.legend(title='CV Method')\n","plt.show()\n","\n","# Plotting Accuracy Comparison\n","plt.figure(figsize=(14, 6))\n","accuracy_results_melted = timing_results.melt(id_vars=['Model'],\n","                                            value_vars=['Hold-Out Test Acc', 'K-Fold Mean Acc',\n","                                                       'Stratified K-Fold Mean Acc', 'LOOCV Mean Acc'],\n","                                            var_name='Method', value_name='Accuracy')\n","sns.barplot(x='Model', y='Accuracy', hue='Method', data=accuracy_results_melted)\n","plt.title('Accuracy Comparison Across Methods')\n","plt.ylabel('Accuracy')\n","plt.ylim(0, 1)\n","plt.xticks(rotation=45)\n","plt.legend(title='Evaluation Method')\n","plt.show()\n","\n","# Print comprehensive results\n","print(\"\\n=== Comprehensive Results ===\")\n","print(timing_results.to_string())"]},{"cell_type":"markdown","metadata":{"id":"a-bOzK3_Bh_v"},"source":["# New Section"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score\n","from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut, KFold, cross_val_score\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","import time\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.applications import MobileNet\n","from sklearn.preprocessing import StandardScaler\n","\n","# Set random seed for reproducibility\n","np.random.seed(32)\n","tf.random.set_seed(32)\n","\n","# Load and preprocess data\n","df = pd.read_csv(\"/content/lung cancer.csv\")\n","df['GENDER'] = df['GENDER'].replace({'M':0, 'F':1})\n","df['LUNG_CANCER'] = df['LUNG_CANCER'].replace({'YES':1, 'NO':0})\n","x = df.drop('LUNG_CANCER', axis=1)\n","y = df['LUNG_CANCER']\n","\n","# Verify dataset shape\n","print(f\"Dataset shape: {df.shape}\")\n","print(f\"Features shape: {x.shape}\")\n","\n","# Split data\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","print(f\"x_train: {x_train.shape}, x_test: {x_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")\n","\n","# Scale features\n","scaler = StandardScaler()\n","x_train_scaled = scaler.fit_transform(x_train)\n","x_test_scaled = scaler.transform(x_test)\n","print(f\"x_train_scaled: {x_train_scaled.shape}, x_test_scaled: {x_test_scaled.shape}\")\n","\n","# Prepare data for CNN/MobileNet\n","n_features = x_train_scaled.shape[1]\n","if n_features != 16:\n","    print(f\"Warning: Expected 16 features, got {n_features}. Adjusting reshape.\")\n","    if n_features < 16:\n","        x_train_scaled = np.pad(x_train_scaled, ((0, 0), (0, 16 - n_features)), mode='constant')\n","        x_test_scaled = np.pad(x_test_scaled, ((0, 0), (0, 16 - n_features)), mode='constant')\n","    else:\n","        x_train_scaled = x_train_scaled[:, :16]\n","        x_test_scaled = x_test_scaled[:, :16]\n","    n_features = 16\n","\n","# Reshape for CNN: 16 features -> 4x4x1\n","x_train_cnn = x_train_scaled.reshape(-1, 4, 4, 1)\n","x_test_cnn = x_test_scaled.reshape(-1, 4, 4, 1)\n","print(f\"x_train_cnn: {x_train_cnn.shape}, x_test_cnn: {x_test_cnn.shape}\")\n","\n","# Reshape for MobileNet: Upscale to 224x224x3\n","x_train_mn = np.repeat(x_train_cnn, 56, axis=1)  # 4 -> 224\n","x_train_mn = np.repeat(x_train_mn, 56, axis=2)\n","x_train_mn = np.repeat(x_train_mn, 3, axis=3)\n","x_test_mn = np.repeat(x_test_cnn, 56, axis=1)\n","x_test_mn = np.repeat(x_test_mn, 56, axis=2)\n","x_test_mn = np.repeat(x_test_mn, 3, axis=3)\n","print(f\"x_train_mn: {x_train_mn.shape}, x_test_mn: {x_test_mn.shape}\")\n","\n","# Initialize lists\n","models = ['GaussianNB', 'SVC', 'Logistic Regression', 'Decision Tree',\n","          'Random Forest', 'Gradient Boosting', 'XGBoost', 'CNN', 'MobileNet']\n","test_accuracies = []\n","train_accuracies = []\n","skfold_means = []\n","loocv_means = []\n","kf_means = []\n","holdout_times = []\n","skfold_times = []\n","loocv_times = []\n","kf_times = []\n","y_pred_list = []\n","model_instances = []\n","\n","# CNN training function\n","def train_cnn(x_train, y_train, x_test, y_test):\n","    model = Sequential([\n","        Conv2D(16, (2, 2), activation='relu', input_shape=(4, 4, 1)),\n","        MaxPooling2D((2, 2)),\n","        Flatten(),\n","        Dense(32, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    start_time = time.time()\n","    history = model.fit(x_train, y_train, epochs=10, batch_size=16, validation_data=(x_test, y_test), verbose=0)\n","    training_time = time.time() - start_time\n","    train_acc = history.history['accuracy'][-1]\n","    test_acc = model.evaluate(x_test, y_test)[1]\n","    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n","    return model, test_acc, train_acc, training_time, y_pred\n","\n","# MobileNet training function\n","def train_mobilenet(x_train, y_train, x_test, y_test):\n","    base_model = MobileNet(weights=None, include_top=False, input_shape=(224, 224, 3))\n","    model = Sequential([\n","        base_model,\n","        Flatten(),\n","        Dense(64, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    start_time = time.time()\n","    history = model.fit(x_train, y_train, epochs=5, batch_size=16, validation_data=(x_test, y_test), verbose=0)\n","    training_time = time.time() - start_time\n","    train_acc = history.history['accuracy'][-1]\n","    test_acc = model.evaluate(x_test, y_test)[1]\n","    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n","    return model, test_acc, train_acc, training_time, y_pred\n","\n","# Function to measure cross-validation time\n","def measure_cv_time(model, x, y, cv_method, cv_params, scoring='accuracy'):\n","    start_time = time.time()\n","    scores = cross_val_score(model, x, y, cv=cv_method(**cv_params), scoring=scoring, n_jobs=-1)\n","    return time.time() - start_time, scores.mean()\n","\n","# Train models and measure times\n","for model_name in models:\n","    print(f\"\\n=== Training {model_name} ===\")\n","\n","    # Initialize model\n","    if model_name == 'GaussianNB':\n","        model = GaussianNB()\n","    elif model_name == 'SVC':\n","        model = SVC(probability=True)\n","    elif model_name == 'Logistic Regression':\n","        model = LogisticRegression()\n","    elif model_name == 'Decision Tree':\n","        model = DecisionTreeClassifier()\n","    elif model_name == 'Random Forest':\n","        model = RandomForestClassifier()\n","    elif model_name == 'Gradient Boosting':\n","        model = GradientBoostingClassifier()\n","    elif model_name == 'XGBoost':\n","        model = XGBClassifier()\n","    elif model_name == 'CNN':\n","        model, test_acc, train_acc, holdout_time, y_pred = train_cnn(x_train_cnn, y_train, x_test_cnn, y_test)\n","        holdout_times.append(holdout_time)\n","        train_accuracies.append(train_acc)\n","        test_accuracies.append(test_acc)\n","        y_pred_list.append(y_pred)\n","        model_instances.append(model)\n","        kf_times.append(0)\n","        skfold_times.append(0)\n","        loocv_times.append(0)\n","        kf_means.append(0)\n","        skfold_means.append(0)\n","        loocv_means.append(0)\n","        print(f\"Hold-out time: {holdout_time:.4f}s\")\n","        continue\n","    elif model_name == 'MobileNet':\n","        model, test_acc, train_acc, holdout_time, y_pred = train_mobilenet(x_train_mn, y_train, x_test_mn, y_test)\n","        holdout_times.append(holdout_time)\n","        train_accuracies.append(train_acc)\n","        test_accuracies.append(test_acc)\n","        y_pred_list.append(y_pred)\n","        model_instances.append(model)\n","        kf_times.append(0)\n","        skfold_times.append(0)\n","        loocv_times.append(0)\n","        kf_means.append(0)\n","        skfold_means.append(0)\n","        loocv_means.append(0)\n","        print(f\"Hold-out time: {holdout_time:.4f}s\")\n","        continue\n","\n","    # Hold-out method\n","    start_time = time.time()\n","    model.fit(x_train_scaled, y_train)\n","    holdout_time = time.time() - start_time\n","    holdout_times.append(holdout_time)\n","\n","    # Predictions\n","    train_acc = model.score(x_train_scaled, y_train)\n","    test_acc = model.score(x_test_scaled, y_test)\n","    y_pred = model.predict(x_test_scaled)\n","\n","    train_accuracies.append(train_acc)\n","    test_accuracies.append(test_acc)\n","    y_pred_list.append(y_pred)\n","    model_instances.append(model)\n","\n","    # Cross-validation\n","    # K-Fold\n","    kf_time, kf_mean = measure_cv_time(model, x, y, KFold, {'n_splits': 5}, scoring='accuracy')\n","    kf_times.append(kf_time)\n","    kf_means.append(kf_mean)\n","\n","    # Stratified K-Fold\n","    skf_time, skf_mean = measure_cv_time(model, x, y, StratifiedKFold, {'n_splits': 5}, scoring='accuracy')\n","    skfold_times.append(skf_time)\n","    skfold_means.append(skf_mean)\n","\n","    # LOOCV (use 10-fold for SVC, Random Forest, Gradient Boosting, XGBoost)\n","    if model_name in ['SVC', 'Random Forest', 'Gradient Boosting', 'XGBoost']:\n","        loo_time, loo_mean = measure_cv_time(model, x, y, KFold, {'n_splits': 10}, scoring='accuracy')\n","    else:\n","        loo_time, loo_mean = measure_cv_time(model, x, y, LeaveOneOut, {}, scoring='accuracy')\n","    loocv_times.append(loo_time)\n","    loocv_means.append(loo_mean)\n","\n","    print(f\"Hold-out time: {holdout_time:.4f}s\")\n","    print(f\"K-Fold time: {kf_time:.4f}s, mean: {kf_mean:.4f}\")\n","    print(f\"Stratified K-Fold time: {skf_time:.4f}s, mean: {skf_mean:.4f}\")\n","    print(f\"LOOCV time: {loo_time:.4f}s, mean: {loo_mean:.4f}\")\n","\n","# Create timing results DataFrame\n","timing_results = pd.DataFrame({\n","    'Model': models,\n","    'Hold-Out Time (s)': holdout_times,\n","    'K-Fold Time (s)': kf_times,\n","    'Stratified K-Fold Time (s)': skfold_times,\n","    'LOOCV Time (s)': loocv_times,\n","    'Hold-Out Train Acc': train_accuracies,\n","    'Hold-Out Test Acc': test_accuracies,\n","    'K-Fold Mean Acc': kf_means,\n","    'Stratified K-Fold Mean Acc': skfold_means,\n","    'LOOCV Mean Acc': loocv_means\n","})\n","\n","# Plotting Training Times (Hold-Out)\n","plt.figure(figsize=(14, 6))\n","sns.barplot(x='Model', y='Hold-Out Time (s)', data=timing_results)\n","plt.title('Training Times (Hold-Out Method)')\n","plt.ylabel('Time (seconds)')\n","plt.xticks(rotation=45)\n","plt.show()\n","\n","# Plotting Cross-Validation Times\n","plt.figure(figsize=(14, 6))\n","timing_results_melted = timing_results.melt(id_vars=['Model'],\n","                                           value_vars=['K-Fold Time (s)', 'Stratified K-Fold Time (s)', 'LOOCV Time (s)'],\n","                                           var_name='CV Method', value_name='Time (s)')\n","sns.barplot(x='Model', y='Time (s)', hue='CV Method', data=timing_results_melted)\n","plt.title('Cross-Validation Times by Method')\n","plt.ylabel('Time (seconds)')\n","plt.xticks(rotation=45)\n","plt.legend(title='CV Method')\n","plt.show()\n","\n","# Plotting Training and Testing Accuracies\n","plt.figure(figsize=(14, 6))\n","x = np.arange(len(models))\n","width = 0.35\n","fig, ax = plt.subplots(figsize=(14, 6))\n","rects1 = ax.bar(x - width/2, train_accuracies, width, label='Training Accuracy')\n","rects2 = ax.bar(x + width/2, test_accuracies, width, label='Testing Accuracy')\n","ax.set_ylabel('Accuracy')\n","ax.set_title('Training and Testing Accuracies of Different Models')\n","ax.set_xticks(x)\n","ax.set_xticklabels(models)\n","ax.legend()\n","plt.xticks(rotation=45)\n","plt.ylim(0, 1)\n","plt.show()\n","\n","# Plotting Accuracy Comparison\n","plt.figure(figsize=(14, 6))\n","accuracy_results_melted = timing_results.melt(id_vars=['Model'],\n","                                             value_vars=['Hold-Out Test Acc', 'K-Fold Mean Acc',\n","                                                        'Stratified K-Fold Mean Acc', 'LOOCV Mean Acc'],\n","                                             var_name='Method', value_name='Accuracy')\n","sns.barplot(x='Model', y='Accuracy', hue='Method', data=accuracy_results_melted)\n","plt.title('Accuracy Comparison Across Methods')\n","plt.ylabel('Accuracy')\n","plt.ylim(0, 1)\n","plt.xticks(rotation=45)\n","plt.legend(title='Evaluation Method')\n","plt.show()\n","\n","# ROC Curve\n","plt.figure(figsize=(12, 6))\n","for model, y_pred, name in zip(model_instances, y_pred_list, models):\n","    if name in ['CNN', 'MobileNet']:\n","        fpr, tpr, _ = roc_curve(y_test, y_pred)\n","    else:\n","        fpr, tpr, _ = roc_curve(y_test, model.predict_proba(x_test_scaled)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(x_test_scaled))\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","# Print comprehensive results\n","print(\"\\n=== Comprehensive Results ===\")\n","print(timing_results.to_string())\n","\n","# Best model based on hold-out test accuracy\n","best_model = timing_results.loc[timing_results['Hold-Out Test Acc'].idxmax()]\n","print(\"\\n=== Best Model ===\")\n","print(best_model)"],"metadata":{"id":"iaY1IV6DDZu_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"277ab2a5-234c-40d2-9416-4c273c07b182"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-1-d38022df77ec>:26: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df['GENDER'] = df['GENDER'].replace({'M':0, 'F':1})\n","<ipython-input-1-d38022df77ec>:27: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df['LUNG_CANCER'] = df['LUNG_CANCER'].replace({'YES':1, 'NO':0})\n"]},{"output_type":"stream","name":"stdout","text":["Dataset shape: (5871, 16)\n","Features shape: (5871, 15)\n","x_train: (4696, 15), x_test: (1175, 15), y_train: (4696,), y_test: (1175,)\n","x_train_scaled: (4696, 15), x_test_scaled: (1175, 15)\n","Warning: Expected 16 features, got 15. Adjusting reshape.\n","x_train_cnn: (4696, 4, 4, 1), x_test_cnn: (1175, 4, 4, 1)\n","x_train_mn: (4696, 224, 224, 3), x_test_mn: (1175, 224, 224, 3)\n","\n","=== Training GaussianNB ===\n","Hold-out time: 0.0098s\n","K-Fold time: 2.4299s, mean: 0.9061\n","Stratified K-Fold time: 0.0523s, mean: 0.9061\n","LOOCV time: 40.0569s, mean: 0.9061\n","\n","=== Training SVC ===\n","Hold-out time: 0.5599s\n","K-Fold time: 9.7953s, mean: 0.8738\n","Stratified K-Fold time: 9.7922s, mean: 0.8738\n","LOOCV time: 21.9747s, mean: 0.8738\n","\n","=== Training Logistic Regression ===\n","Hold-out time: 0.0351s\n","K-Fold time: 0.3611s, mean: 0.9457\n","Stratified K-Fold time: 0.4027s, mean: 0.9443\n","LOOCV time: 348.1221s, mean: 0.9452\n","\n","=== Training Decision Tree ===\n","Hold-out time: 0.0056s\n","K-Fold time: 0.1133s, mean: 0.9968\n","Stratified K-Fold time: 0.0601s, mean: 0.9968\n","LOOCV time: 51.7759s, mean: 0.9935\n","\n","=== Training Random Forest ===\n","Hold-out time: 0.2656s\n","K-Fold time: 1.1615s, mean: 0.9968\n","Stratified K-Fold time: 1.1213s, mean: 0.9968\n","LOOCV time: 2.1851s, mean: 0.9968\n","\n","=== Training Gradient Boosting ===\n","Hold-out time: 0.4405s\n","K-Fold time: 2.7738s, mean: 0.9889\n","Stratified K-Fold time: 1.9127s, mean: 0.9910\n","LOOCV time: 3.6190s, mean: 0.9891\n","\n","=== Training XGBoost ===\n","Hold-out time: 0.1072s\n","K-Fold time: 0.4453s, mean: 0.9968\n","Stratified K-Fold time: 0.3779s, mean: 0.9968\n","LOOCV time: 0.7744s, mean: 0.9968\n","\n","=== Training CNN ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9336 - loss: 0.1699\n","\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","Hold-out time: 11.6002s\n","\n","=== Training MobileNet ===\n"]}]},{"cell_type":"markdown","source":["#start"],"metadata":{"id":"9Xd0pmd5Mxj2"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import roc_curve, auc, accuracy_score\n","from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut, KFold, cross_val_score\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.applications import MobileNet\n","from sklearn.preprocessing import StandardScaler\n","from transformers import TFSwinModel\n","\n","# Set random seeds for reproducibility\n","np.random.seed(32)\n","tf.random.set_seed(32)\n","\n","# Load and preprocess data\n","def load_data(file_path):\n","    df = pd.read_csv(file_path)\n","    # Encode categorical variables\n","    df['GENDER'] = df['GENDER'].replace({'M': 0, 'F': 1})\n","    df['LUNG_CANCER'] = df['LUNG_CANCER'].replace({'YES': 1, 'NO': 0})\n","    X = df.drop('LUNG_CANCER', axis=1)\n","    y = df['LUNG_CANCER']\n","    print(f\"Dataset shape: {df.shape}, Features shape: {X.shape}, Target shape: {y.shape}\")\n","    return X, y\n","\n","# Prepare data for training\n","def prepare_data(X, y, test_size=0.2):\n","    # Train-test split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n","    print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")\n","\n","    # Scale features\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_test_scaled = scaler.transform(X_test)\n","\n","    # Reshape for CNN: Assume 16 features (4x4x1)\n","    n_features = X_train_scaled.shape[1]\n","    if n_features != 16:\n","        print(f\"Warning: Expected 16 features, got {n_features}. Adjusting...\")\n","        if n_features < 16:\n","            X_train_scaled = np.pad(X_train_scaled, ((0, 0), (0, 16 - n_features)), mode='constant')\n","            X_test_scaled = np.pad(X_test_scaled, ((0, 0), (0, 16 - n_features)), mode='constant')\n","        else:\n","            X_train_scaled = X_train_scaled[:, :16]\n","            X_test_scaled = X_test_scaled[:, :16]\n","        n_features = 16\n","\n","    X_train_cnn = X_train_scaled.reshape(-1, 4, 4, 1)\n","    X_test_cnn = X_test_scaled.reshape(-1, 4, 4, 1)\n","    print(f\"X_train_cnn: {X_train_cnn.shape}, X_test_cnn: {X_test_cnn.shape}\")\n","\n","    # Reshape for MobileNet/Swin: Upscale to 224x224x3\n","    X_train_mn = np.repeat(X_train_cnn, 56, axis=1)  # 4 -> 224\n","    X_train_mn = np.repeat(X_train_mn, 56, axis=2)\n","    X_train_mn = np.repeat(X_train_mn, 3, axis=3)\n","    X_test_mn = np.repeat(X_test_cnn, 56, axis=1)\n","    X_test_mn = np.repeat(X_test_mn, 56, axis=2)\n","    X_test_mn = np.repeat(X_test_mn, 3, axis=3)\n","    print(f\"X_train_mn: {X_train_mn.shape}, X_test_mn: {X_test_mn.shape}\")\n","\n","    return X_train_scaled, X_test_scaled, X_train_cnn, X_test_cnn, X_train_mn, X_test_mn, y_train, y_test\n","\n","# CNN model\n","def train_cnn(X_train, y_train, X_test, y_test):\n","    model = Sequential([\n","        Conv2D(16, (2, 2), activation='relu', input_shape=(4, 4, 1)),\n","        MaxPooling2D((2, 2)),\n","        Flatten(),\n","        Dense(32, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=0)\n","    train_acc = history.history['accuracy'][-1]\n","    test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n","    y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(\"int32\").flatten()\n","    return model, train_acc, test_acc, y_pred\n","\n","# MobileNet model\n","def train_mobilenet(X_train, y_train, X_test, y_test):\n","    base_model = MobileNet(weights=None, include_top=False, input_shape=(224, 224, 3))\n","    model = Sequential([\n","        base_model,\n","        Flatten(),\n","        Dense(64, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    history = model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_test, y_test), verbose=0)\n","    train_acc = history.history['accuracy'][-1]\n","    test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n","    y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(\"int32\").flatten()\n","    return model, train_acc, test_acc, y_pred\n","\n","# Swin Transformer model\n","def train_swin_transformer(X_train, y_train, X_test, y_test):\n","    try:\n","        base_model = TFSwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n","        inputs = tf.keras.Input(shape=(224, 224, 3))\n","        x = base_model(inputs).pooler_output\n","        x = Dense(64, activation='relu')(x)\n","        x = Dropout(0.5)(x)\n","        outputs = Dense(1, activation='sigmoid')(x)\n","        model = tf.keras.Model(inputs, outputs)\n","        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","        history = model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_test, y_test), verbose=0)\n","        train_acc = history.history['accuracy'][-1]\n","        test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n","        y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(\"int32\").flatten()\n","        return model, train_acc, test_acc, y_pred\n","    except Exception as e:\n","        print(f\"Error training Swin Transformer: {e}\")\n","        return None, 0, 0, np.zeros_like(y_test)\n","\n","# Train and evaluate all models\n","def train_and_evaluate(X, y, X_train_scaled, X_test_scaled, X_train_cnn, X_test_cnn, X_train_mn, X_test_mn, y_train, y_test):\n","    models = [\n","        ('GaussianNB', GaussianNB()),\n","        ('SVC', SVC(probability=True)),\n","        ('Logistic Regression', LogisticRegression()),\n","        ('Decision Tree', DecisionTreeClassifier()),\n","        ('Random Forest', RandomForestClassifier()),\n","        ('Gradient Boosting', GradientBoostingClassifier()),\n","        ('XGBoost', XGBClassifier())\n","    ]\n","    model_names = [name for name, _ in models] + ['CNN', 'MobileNet', 'Swin Transformer']\n","    train_accuracies = []\n","    test_accuracies = []\n","    kf_means = []\n","    skf_means = []\n","    loo_means = []\n","    y_pred_list = []\n","    model_instances = []\n","\n","    # Train traditional ML models\n","    for name, model in models:\n","        print(f\"\\nTraining {name}...\")\n","        # Hold-Out\n","        model.fit(X_train_scaled, y_train)\n","        train_acc = model.score(X_train_scaled, y_train)\n","        test_acc = model.score(X_test_scaled, y_test)\n","        y_pred = model.predict(X_test_scaled)\n","\n","        # Cross-validation\n","        kf_scores = cross_val_score(model, X, y, cv=KFold(n_splits=5), scoring='accuracy', n_jobs=-1)\n","        skf_scores = cross_val_score(model, X, y, cv=StratifiedKFold(n_splits=5), scoring='accuracy', n_jobs=-1)\n","        if name in ['SVC', 'Random Forest', 'Gradient Boosting', 'XGBoost']:\n","            loo_scores = cross_val_score(model, X, y, cv=KFold(n_splits=10), scoring='accuracy', n_jobs=-1)\n","        else:\n","            loo_scores = cross_val_score(model, X, y, cv=LeaveOneOut(), scoring='accuracy', n_jobs=-1)\n","\n","        # Store results\n","        train_accuracies.append(train_acc)\n","        test_accuracies.append(test_acc)\n","        kf_means.append(kf_scores.mean())\n","        skf_means.append(skf_scores.mean())\n","        loo_means.append(loo_scores.mean())\n","        y_pred_list.append(y_pred)\n","        model_instances.append(model)\n","\n","        print(f\"Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n","        print(f\"K-Fold Mean: {kf_scores.mean():.4f}\")\n","        print(f\"Stratified K-Fold Mean: {skf_scores.mean():.4f}\")\n","        print(f\"LOOCV Mean: {loo_scores.mean():.4f}\")\n","\n","    # Train CNN\n","    print(\"\\nTraining CNN...\")\n","    cnn_model, cnn_train_acc, cnn_test_acc, cnn_y_pred = train_cnn(X_train_cnn, y_train, X_test_cnn, y_test)\n","    train_accuracies.append(cnn_train_acc)\n","    test_accuracies.append(cnn_test_acc)\n","    kf_means.append(0)  # No CV for deep learning\n","    skf_means.append(0)\n","    loo_means.append(0)\n","    y_pred_list.append(cnn_y_pred)\n","    model_instances.append(cnn_model)\n","    print(f\"Train Acc: {cnn_train_acc:.4f}, Test Acc: {cnn_test_acc:.4f}\")\n","\n","    # Train MobileNet\n","    print(\"\\nTraining MobileNet...\")\n","    mn_model, mn_train_acc, mn_test_acc, mn_y_pred = train_mobilenet(X_train_mn, y_train, X_test_mn, y_test)\n","    train_accuracies.append(mn_train_acc)\n","    test_accuracies.append(mn_test_acc)\n","    kf_means.append(0)\n","    skf_means.append(0)\n","    loo_means.append(0)\n","    y_pred_list.append(mn_y_pred)\n","    model_instances.append(mn_model)\n","    print(f\"Train Acc: {mn_train_acc:.4f}, Test Acc: {mn_test_acc:.4f}\")\n","\n","    # Train Swin Transformer\n","    print(\"\\nTraining Swin Transformer...\")\n","    swin_model, swin_train_acc, swin_test_acc, swin_y_pred = train_swin_transformer(X_train_mn, y_train, X_test_mn, y_test)\n","    train_accuracies.append(swin_train_acc)\n","    test_accuracies.append(swin_test_acc)\n","    kf_means.append(0)\n","    skf_means.append(0)\n","    loo_means.append(0)\n","    y_pred_list.append(swin_y_pred)\n","    model_instances.append(swin_model)\n","    print(f\"Train Acc: {swin_train_acc:.4f}, Test Acc: {swin_test_acc:.4f}\")\n","\n","    # Create results DataFrame\n","    results = pd.DataFrame({\n","        'Model': model_names,\n","        'Hold-Out Train Acc': train_accuracies,\n","        'Hold-Out Test Acc': test_accuracies,\n","        'K-Fold Mean Acc': kf_means,\n","        'Stratified K-Fold Mean Acc': skf_means,\n","        'LOOCV Mean Acc': loo_means\n","    })\n","    return results, model_instances, y_pred_list\n","\n","# Plotting functions\n","def plot_accuracies(results):\n","    plt.figure(figsize=(14, 6))\n","    x = np.arange(len(results['Model']))\n","    width = 0.35\n","    plt.bar(x - width/2, results['Hold-Out Train Acc'], width, label='Training Accuracy')\n","    plt.bar(x + width/2, results['Hold-Out Test Acc'], width, label='Testing Accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.title('Training and Testing Accuracies')\n","    plt.xticks(x, results['Model'], rotation=45)\n","    plt.legend()\n","    plt.ylim(0, 1)\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_accuracy_comparison(results):\n","    plt.figure(figsize=(14, 6))\n","    melted = results.melt(id_vars=['Model'],\n","                         value_vars=['Hold-Out Test Acc', 'K-Fold Mean Acc',\n","                                    'Stratified K-Fold Mean Acc', 'LOOCV Mean Acc'],\n","                         var_name='Method', value_name='Accuracy')\n","    sns.barplot(x='Model', y='Accuracy', hue='Method', data=melted)\n","    plt.title('Accuracy Comparison Across Methods')\n","    plt.ylabel('Accuracy')\n","    plt.xticks(rotation=45)\n","    plt.ylim(0, 1)\n","    plt.legend(title='Evaluation Method')\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_roc_curves(model_instances, y_pred_list, X_test_scaled, X_test_cnn, X_test_mn, y_test, model_names):\n","    plt.figure(figsize=(12, 6))\n","    for i, (model, y_pred, name) in enumerate(zip(model_instances, y_pred_list, model_names)):\n","        if name == 'CNN':\n","            fpr, tpr, _ = roc_curve(y_test, y_pred)\n","        elif name == 'MobileNet':\n","            fpr, tpr, _ = roc_curve(y_test, y_pred)\n","        elif name == 'Swin Transformer':\n","            fpr, tpr, _ = roc_curve(y_test, y_pred)\n","        else:\n","            if hasattr(model, \"predict_proba\"):\n","                y_scores = model.predict_proba(X_test_scaled)[:, 1]\n","            else:\n","                y_scores = model.predict(X_test_scaled)\n","            fpr, tpr, _ = roc_curve(y_test, y_scores)\n","        roc_auc = auc(fpr, tpr)\n","        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('ROC Curve')\n","    plt.legend(loc='lower right')\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Main execution\n","def main():\n","    # Load data\n","    file_path = \"/content/lung cancer.csv\"  # Update with actual path\n","    X, y = load_data(file_path)\n","\n","    # Prepare data\n","    X_train_scaled, X_test_scaled, X_train_cnn, X_test_cnn, X_train_mn, X_test_mn, y_train, y_test = prepare_data(X, y)\n","\n","    # Train and evaluate\n","    results, model_instances, y_pred_list = train_and_evaluate(\n","        X, y, X_train_scaled, X_test_scaled, X_train_cnn, X_test_cnn, X_train_mn, X_test_mn, y_train, y_test\n","    )\n","\n","    # Plot results\n","    plot_accuracies(results)\n","    plot_accuracy_comparison(results)\n","    plot_roc_curves(model_instances, y_pred_list, X_test_scaled, X_test_cnn, X_test_mn, y_test, results['Model'])\n","\n","    # Print results\n","    print(\"\\n=== Comprehensive Results ===\")\n","    print(results.to_string(index=False))\n","\n","    # Best model\n","    best_model = results.loc[results['Hold-Out Test Acc'].idxmax()]\n","    print(\"\\n=== Best Model ===\")\n","    print(best_model)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-a2t13JNM1mf","outputId":"68d98544-2382-4183-bcb1-70f5bf9bc59b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-009c040a1a61>:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df['GENDER'] = df['GENDER'].replace({'M': 0, 'F': 1})\n","<ipython-input-1-009c040a1a61>:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df['LUNG_CANCER'] = df['LUNG_CANCER'].replace({'YES': 1, 'NO': 0})\n"]},{"output_type":"stream","name":"stdout","text":["Dataset shape: (5871, 16), Features shape: (5871, 15), Target shape: (5871,)\n","X_train: (4696, 15), X_test: (1175, 15), y_train: (4696,), y_test: (1175,)\n","Warning: Expected 16 features, got 15. Adjusting...\n","X_train_cnn: (4696, 4, 4, 1), X_test_cnn: (1175, 4, 4, 1)\n","X_train_mn: (4696, 224, 224, 3), X_test_mn: (1175, 224, 224, 3)\n","\n","Training GaussianNB...\n","Train Acc: 0.9069, Test Acc: 0.9030\n","K-Fold Mean: 0.9061\n","Stratified K-Fold Mean: 0.9061\n","LOOCV Mean: 0.9061\n","\n","Training SVC...\n","Train Acc: 0.9847, Test Acc: 0.9804\n","K-Fold Mean: 0.8738\n","Stratified K-Fold Mean: 0.8738\n","LOOCV Mean: 0.8738\n","\n","Training Logistic Regression...\n","Train Acc: 0.9461, Test Acc: 0.9404\n","K-Fold Mean: 0.9457\n","Stratified K-Fold Mean: 0.9443\n","LOOCV Mean: 0.9452\n","\n","Training Decision Tree...\n","Train Acc: 0.9968, Test Acc: 0.9966\n","K-Fold Mean: 0.9968\n","Stratified K-Fold Mean: 0.9968\n","LOOCV Mean: 0.9935\n","\n","Training Random Forest...\n","Train Acc: 0.9968, Test Acc: 0.9966\n","K-Fold Mean: 0.9968\n","Stratified K-Fold Mean: 0.9968\n","LOOCV Mean: 0.9968\n","\n","Training Gradient Boosting...\n","Train Acc: 0.9842, Test Acc: 0.9821\n","K-Fold Mean: 0.9889\n","Stratified K-Fold Mean: 0.9910\n","LOOCV Mean: 0.9891\n","\n","Training XGBoost...\n","Train Acc: 0.9968, Test Acc: 0.9966\n","K-Fold Mean: 0.9968\n","Stratified K-Fold Mean: 0.9968\n","LOOCV Mean: 0.9968\n","\n","Training CNN...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Train Acc: 0.9342, Test Acc: 0.9268\n","\n","Training MobileNet...\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5Y29UO2paT_0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNWZxaCiePlnQwLtr0puE9+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}